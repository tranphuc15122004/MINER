# MINER - Multi-Interest News Recommendation

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.1.0-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

Triá»ƒn khai mÃ´ hÃ¬nh **MINER (Multi-Interest Network for News Recommendation)** dá»±a trÃªn paper [ACL 2022](https://aclanthology.org/2022.findings-acl.29.pdf), Ã¡p dá»¥ng cho bÃ i toÃ¡n gá»£i Ã½ tin tá»©c trÃªn MIND dataset.

## ğŸ“‹ Má»¥c lá»¥c

- [Tá»•ng quan](#-tá»•ng-quan)
- [Kiáº¿n trÃºc mÃ´ hÃ¬nh](#-kiáº¿n-trÃºc-mÃ´-hÃ¬nh)
- [CÃ i Ä‘áº·t](#-cÃ i-Ä‘áº·t)
- [Chuáº©n bá»‹ dá»¯ liá»‡u](#-chuáº©n-bá»‹-dá»¯-liá»‡u)
- [Sá»­ dá»¥ng](#-sá»­-dá»¥ng)
  - [Training](#1-training)
  - [Evaluation](#2-evaluation)
  - [Submission Generation](#3-submission-generation)
- [Ensemble Methods](#-ensemble-methods)
- [Cáº¥u trÃºc thÆ° má»¥c](#-cáº¥u-trÃºc-thÆ°-má»¥c)
- [Káº¿t quáº£](#-káº¿t-quáº£)
- [TÃ i liá»‡u tham kháº£o](#-tÃ i-liá»‡u-tham-kháº£o)

## ğŸ¯ Tá»•ng quan

MINER lÃ  má»™t mÃ´ hÃ¬nh neural network Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ:
- **Há»c multiple user interests** tá»« lá»‹ch sá»­ Ä‘á»c tin tá»©c
- **Category-aware attention** Ä‘á»ƒ cáº£i thiá»‡n kháº£ nÄƒng Ä‘áº¡i diá»‡n tin tá»©c
- **Multi-interest matching** giá»¯a user history vÃ  candidate news
- **Ensemble predictions** tá»« nhiá»u models Ä‘á»ƒ tá»‘i Æ°u hiá»‡u suáº¥t

### TÃ­nh nÄƒng chÃ­nh

âœ… **News Encoder**: Sá»­ dá»¥ng pre-trained language model (RoBERTa/DistilRoBERTa) Ä‘á»ƒ encode title vÃ  sapo  
âœ… **Category Embedding**: TÃ­ch há»£p thÃ´ng tin danh má»¥c tin tá»©c  
âœ… **Multi-Head Attention**: Há»c K interests khÃ¡c nhau cá»§a user  
âœ… **Flexible Scoring**: Há»— trá»£ nhiá»u phÆ°Æ¡ng phÃ¡p tá»•ng há»£p scores (mean, max, weighted)  
âœ… **Ensemble Learning**: Weighted Mean & Stacking ensemble  
âœ… **Production-ready**: Há»— trá»£ inference mode khÃ´ng cáº§n ground truth  

## ğŸ—ï¸ Kiáº¿n trÃºc mÃ´ hÃ¬nh

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      MINER Architecture                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  News Encoder (Title + Sapo)                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚  RoBERTa     â”‚ â”€â”€â”€â”€â”€â”€> â”‚  Linear      â”‚                  â”‚
â”‚  â”‚  Embedding   â”‚         â”‚  Projection  â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚         â”‚                         â”‚                          â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                       â–¼                                      â”‚
â”‚              Category Attention                              â”‚
â”‚              (Optional bias)                                 â”‚
â”‚                       â”‚                                      â”‚
â”‚                       â–¼                                      â”‚
â”‚            Multi-Interest User Encoder                       â”‚
â”‚         (K attention heads â†’ K interests)                    â”‚
â”‚                       â”‚                                      â”‚
â”‚                       â–¼                                      â”‚
â”‚           Interest-Candidate Matching                        â”‚
â”‚          (Cosine similarity Ã— K scores)                      â”‚
â”‚                       â”‚                                      â”‚
â”‚                       â–¼                                      â”‚
â”‚            Score Aggregation (mean/max/weighted)             â”‚
â”‚                       â”‚                                      â”‚
â”‚                       â–¼                                      â”‚
â”‚                 Click Probability                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ CÃ i Ä‘áº·t

### YÃªu cáº§u há»‡ thá»‘ng

- Python >= 3.8
- CUDA (khuyáº¿n nghá»‹ cho training)
- RAM: >= 16GB
- Disk: >= 50GB (cho MIND dataset)

### CÃ i Ä‘áº·t dependencies

```bash
# Clone repository
git clone https://github.com/tranphuc15122004/MINER.git
cd MINER

# Táº¡o virtual environment (khuyáº¿n nghá»‹)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# hoáº·c
venv\Scripts\activate  # Windows

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt
```

### Dependencies chÃ­nh

- `torch==2.1.0` - Deep learning framework
- `transformers==4.37.2` - Pre-trained language models
- `scikit-learn==1.3.2` - Machine learning utilities
- `pandas==2.1.3` - Data processing
- `tensorboard==2.15.1` - Training visualization

## ğŸ“Š Chuáº©n bá»‹ dá»¯ liá»‡u

### 1. Download MIND dataset

```bash
# Download MINDlarge dataset
# Train set
wget https://mind201910.blob.core.windows.net/release/MINDlarge_train.zip
unzip MINDlarge_train.zip -d data/MINDlarge_train/

# Dev set
wget https://mind201910.blob.core.windows.net/release/MINDlarge_dev.zip
unzip MINDlarge_dev.zip -d data/MINDlarge_dev/

# Test set
wget https://mind201910.blob.core.windows.net/release/MINDlarge_test.zip
unzip MINDlarge_test.zip -d data/MINDlarge_test/
```

### 2. Chuáº©n bá»‹ mappings

```bash
# Táº¡o user2id vÃ  category2id mappings
python prepare_mind_mappings.py \
    --train_behaviors data/MINDlarge_train/MINDlarge_train/behaviors.tsv \
    --train_news data/MINDlarge_train/MINDlarge_train/news.tsv \
    --output_dir data/
```

Káº¿t quáº£ táº¡o ra:
- `data/user2id.json` - Mapping user ID sang integer
- `data/category2id.json` - Mapping category sang integer

## ğŸš€ Sá»­ dá»¥ng

### 1. Training

Sá»­ dá»¥ng file config hoáº·c command line arguments:

#### Option A: Sá»­ dá»¥ng config file

```bash
python main.py train @config/train.txt
```

#### Option B: Command line arguments

```bash
python main.py train \
    --model_name miner_base \
    --pretrained_embedding "distilroberta-base" \
    --pretrained_tokenizer "distilroberta-base" \
    --user2id_path data/user2id.json \
    --category2id_path data/category2id.json \
    --train_behaviors_path data/train/behaviors.tsv \
    --train_news_path data/train/news.tsv \
    --eval_behaviors_path data/valid/behaviors.tsv \
    --eval_news_path data/valid/news.tsv \
    --max_title_length 30 \
    --max_sapo_length 60 \
    --his_length 50 \
    --num_context_codes 20 \
    --context_code_dim 200 \
    --train_batch_size 32 \
    --eval_batch_size 64 \
    --npratio 4 \
    --epochs 5 \
    --learning_rate 5e-5 \
    --use_category_bias \
    --use_sapo \
    --metrics auc mrr ndcg@5 ndcg@10
```

#### Tham sá»‘ quan trá»ng:

- `--use_sapo`: Sá»­ dá»¥ng cáº£ sapo (abstract) ngoÃ i title
- `--use_category_bias`: KÃ­ch hoáº¡t category-aware attention
- `--freeze_transformer`: Freeze weights cá»§a pre-trained model
- `--apply_reduce_dim`: Giáº£m chiá»u embedding cá»§a RoBERTa
- `--num_context_codes`: Sá»‘ lÆ°á»£ng interests (K)
- `--score_type`: CÃ¡ch tá»•ng há»£p scores (mean/max/weighted)

### 2. Evaluation

```bash
python main.py eval @config/eval.txt

# hoáº·c
python main.py eval \
    --saved_model_path checkpoint/bestAucModel.pt \
    --data_name valid \
    --eval_behaviors_path data/valid/behaviors.tsv \
    --eval_news_path data/valid/news.tsv \
    --eval_batch_size 128 \
    --metrics auc mrr ndcg@5 ndcg@10
```

### 3. Submission Generation

```bash
python main.py submission @config/submission.txt

# hoáº·c
python main.py submission \
    --saved_model_path checkpoint/bestAucModel.pt \
    --data_name test \
    --eval_behaviors_path data/MINDlarge_test/MINDlarge_test/behaviors.tsv \
    --eval_news_path data/MINDlarge_test/MINDlarge_test/news.tsv \
    --eval_batch_size 128
```

Output: File prediction táº¡i `eval/{timestamp}/prediction.txt`

## ğŸ­ Ensemble Methods

Dá»± Ã¡n há»— trá»£ 2 phÆ°Æ¡ng phÃ¡p ensemble Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t:

### 1. Weighted Mean Ensemble

Káº¿t há»£p predictions báº±ng **trá»ng sá»‘ tá»‘i Æ°u** (tÃ¬m báº±ng Bayesian Optimization):

```bash
python phase2/run_ensemble.py \
    --predictions checkpoint/prediction_prod_Ngoc.txt \
                  checkpoint/prediction_prod_Phuc.txt \
                  checkpoint/prediction_prod_Son.txt \
    --truth ref/truth.txt \
    --method weighted \
    --output-dir phase2/ensemble_results \
    --n-trials 500
```

**CÃ¡ch hoáº¡t Ä‘á»™ng:**
- Sá»­ dá»¥ng Optuna Ä‘á»ƒ tÃ¬m trá»ng sá»‘ tá»‘i Æ°u wâ‚, wâ‚‚, ..., wâ‚™
- Maximize impression-level AUC
- Output: `prediction_weighted_prod.txt` vÃ  `prediction_weighted_rank.txt`

### 2. Stacking Ensemble

Sá»­ dá»¥ng **meta-model** (Logistic Regression) há»c cÃ¡ch káº¿t há»£p predictions:

```bash
python phase2/run_ensemble.py \
    --predictions checkpoint/prediction_prod_Ngoc.txt \
                  checkpoint/prediction_prod_Phuc.txt \
                  checkpoint/prediction_prod_Son.txt \
    --truth ref/truth.txt \
    --method stacking \
    --output-dir phase2/ensemble_results
```

**CÃ¡ch hoáº¡t Ä‘á»™ng:**
- Train Logistic Regression trÃªn predictions tá»« base models
- Sá»­ dá»¥ng 5-fold cross-validation
- Output: `prediction_stacking_prod.txt` vÃ  `prediction_stacking_rank.txt`

### Universal Inference (Production Mode)

Cho production environment **khÃ´ng cÃ³ ground truth**:

```bash
python phase2/universal_infer.py \
    --predictions model1_pred.txt model2_pred.txt model3_pred.txt \
    --weighted-dir phase2/ensemble_results/weighted_mean \
    --stacking-dir phase2/ensemble_results/stacking \
    --output-dir results \
    --methods all
```

**TÃ­nh nÄƒng:**
- âœ… KhÃ´ng báº¯t buá»™c truth file
- âœ… Tá»± Ä‘á»™ng detect mode (evaluation vs inference)
- âœ… Há»— trá»£ cáº£ weighted vÃ  stacking
- âœ… Production-ready

### So sÃ¡nh Ensemble Methods

| Method | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm | Use case |
|--------|---------|------------|----------|
| **Weighted Mean** | âš¡ Nhanh, Ä‘Æ¡n giáº£n<br>ğŸ“Š Dá»… interpret weights | ğŸ”¢ Chá»‰ linear combination | Models tÆ°Æ¡ng Ä‘á»“ng nhau |
| **Stacking** | ğŸ¯ Há»c nonlinear patterns<br>ğŸ’ª Robust hÆ¡n | â±ï¸ Cháº­m hÆ¡n<br>ğŸ“ Cáº§n thÃªm data | Models Ä‘a dáº¡ng |

Xem thÃªm chi tiáº¿t táº¡i [ENSEMBLE_THEORY.md](phase2/ENSEMBLE_THEORY.md).

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
MINER/
â”œâ”€â”€ main.py                      # Entry point chÃ­nh
â”œâ”€â”€ arguments.py                 # Äá»‹nh nghÄ©a arguments
â”œâ”€â”€ requirements.txt             # Dependencies
â”‚
â”œâ”€â”€ config/                      # Config files
â”‚   â”œâ”€â”€ train.txt               # Training config
â”‚   â”œâ”€â”€ eval.txt                # Evaluation config
â”‚   â””â”€â”€ submission.txt          # Submission config
â”‚
â”œâ”€â”€ src/                        # Source code
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ model.py           # MINER model
â”‚   â”‚   â””â”€â”€ news_encoder.py   # News encoder
â”‚   â”œâ”€â”€ trainer.py             # Training logic
â”‚   â”œâ”€â”€ reader.py              # Data loading
â”‚   â”œâ”€â”€ evaluation.py          # Metrics
â”‚   â””â”€â”€ utils.py               # Utilities
â”‚
â”œâ”€â”€ data/                       # Dataset
â”‚   â”œâ”€â”€ user2id.json
â”‚   â”œâ”€â”€ category2id.json
â”‚   â”œâ”€â”€ MINDlarge_train/
â”‚   â”œâ”€â”€ MINDlarge_dev/
â”‚   â””â”€â”€ MINDlarge_test/
â”‚
â”œâ”€â”€ checkpoint/                 # Saved models
â”‚   â”œâ”€â”€ bestAucModel.pt
â”‚   â””â”€â”€ finalModel.pt
â”‚
â”œâ”€â”€ phase2/                     # Ensemble methods
â”‚   â”œâ”€â”€ ensemble.py            # Ensemble implementation
â”‚   â”œâ”€â”€ run_ensemble.py        # Run ensemble
â”‚   â”œâ”€â”€ universal_infer.py     # Production inference
â”‚   â”œâ”€â”€ ENSEMBLE_THEORY.md     # Ensemble theory
â”‚   â””â”€â”€ ensemble_results/      # Ensemble outputs
â”‚
â”œâ”€â”€ scripts/                    # Utility scripts
â”‚   â”œâ”€â”€ sub_evaluator.py       # Evaluate predictions
â”‚   â”œâ”€â”€ prod_to_rank.py        # Convert prod â†’ rank
â”‚   â””â”€â”€ generate_truth.py      # Generate truth file
â”‚
â””â”€â”€ train/                      # Training logs
    â””â”€â”€ {timestamp}/
        â”œâ”€â”€ args.json
        â””â”€â”€ *.pt
```

## ğŸ“š TÃ i liá»‡u tham kháº£o

### Papers

1. **MINER**: Li et al. (2022) - [Efficiently Leveraging Multi-level User Intent for Session-based Recommendation via Atten-Mixer Network](https://aclanthology.org/2022.findings-acl.29.pdf)

2. **MIND Dataset**: Wu et al. (2020) - [MIND: A Large-scale Dataset for News Recommendation](https://msnews.github.io/assets/doc/ACL2020_MIND.pdf)

