# L√Ω Thuy·∫øt Ensemble Methods: Weighted Mean & Stacking

T√†i li·ªáu n√†y gi·∫£i th√≠ch chi ti·∫øt v·ªÅ 2 ph∆∞∆°ng ph√°p ensemble ƒë∆∞·ª£c s·ª≠ d·ª•ng trong h·ªá th·ªëng recommendation, d·ª±a tr√™n implementation c·ªßa team sugawarya (RecSys Challenge 2024 Winner).

---

## 1. WEIGHTED MEAN ENSEMBLE

### 1.1. √ù t∆∞·ªüng c∆° b·∫£n

Weighted Mean Ensemble k·∫øt h·ª£p predictions t·ª´ nhi·ªÅu models b·∫±ng c√°ch t√≠nh **trung b√¨nh c√≥ tr·ªçng s·ªë**:

```
P_ensemble = w‚ÇÅ √ó P‚ÇÅ + w‚ÇÇ √ó P‚ÇÇ + ... + w‚Çô √ó P‚Çô
```

Trong ƒë√≥:
- `P·µ¢`: Prediction t·ª´ model th·ª© i
- `w·µ¢`: Tr·ªçng s·ªë c·ªßa model i (0 ‚â§ w·µ¢ ‚â§ 1)
- `P_ensemble`: Prediction cu·ªëi c√πng

### 1.2. V·∫•n ƒë·ªÅ: T√¨m tr·ªçng s·ªë t·ªëi ∆∞u

**C√¢u h·ªèi:** L√†m th·∫ø n√†o ƒë·ªÉ t√¨m b·ªô tr·ªçng s·ªë {w‚ÇÅ, w‚ÇÇ, ..., w‚Çô} t·ªët nh·∫•t?

**Gi·∫£i ph√°p:** S·ª≠ d·ª•ng **Bayesian Optimization** (Optuna library)

### 1.3. Quy tr√¨nh t·ªëi ∆∞u tr·ªçng s·ªë

#### B∆∞·ªõc 1: Data Sampling
```
Validation set ‚Üí Sample 1/100 impressions ‚Üí Mini validation set
```
**L√Ω do:** Gi·∫£m th·ªùi gian t√≠nh to√°n trong qu√° tr√¨nh t·ªëi ∆∞u

#### B∆∞·ªõc 2: Define Objective Function
```python
objective(w‚ÇÅ, w‚ÇÇ, ..., w‚Çô):
    # T√≠nh prediction ensemble
    P_ensemble = w‚ÇÅ√óP‚ÇÅ + w‚ÇÇ√óP‚ÇÇ + ... + w‚Çô√óP‚Çô
    
    # T√≠nh AUC cho t·ª´ng impression
    for each impression:
        auc_i = AUC(y_true_i, P_ensemble_i)
    
    # Return mean AUC
    return mean(auc‚ÇÅ, auc‚ÇÇ, ..., auc‚Çò)
```

**Impression-level AUC:**
- T√≠nh AUC ri√™ng cho T·ª™NG impression (group of candidates)
- Mean AUC = average c·ªßa t·∫•t c·∫£ impression AUCs
- Ph√π h·ª£p v·ªõi c√°ch ƒë√°nh gi√° c·ªßa MIND dataset

#### B∆∞·ªõc 3: Bayesian Optimization
```
Optuna trials:
  Trial 1: Try w‚ÇÅ=0.3, w‚ÇÇ=0.7 ‚Üí AUC = 0.65
  Trial 2: Try w‚ÇÅ=0.5, w‚ÇÇ=0.5 ‚Üí AUC = 0.67
  Trial 3: Try w‚ÇÅ=0.6, w‚ÇÇ=0.4 ‚Üí AUC = 0.68
  ...
  Trial 200: Try w‚ÇÅ=0.58, w‚ÇÇ=0.42 ‚Üí AUC = 0.69 (best)
```

**Optuna advantages:**
- Th√¥ng minh h∆°n Grid Search (kh√¥ng th·ª≠ t·∫•t c·∫£ combinations)
- H·ªçc t·ª´ trials tr∆∞·ªõc ƒë·ªÉ suggest trials m·ªõi t·ªët h∆°n
- Converge nhanh h∆°n Random Search

### 1.4. Prediction v·ªõi weights ƒë√£ t·ªëi ∆∞u

```
For new data:
  P_ensemble = 0.58√óP‚ÇÅ + 0.42√óP‚ÇÇ
```

### 1.5. ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm

**∆Øu ƒëi·ªÉm:**
- ‚úÖ ƒê∆°n gi·∫£n, d·ªÖ hi·ªÉu
- ‚úÖ Nhanh (ch·ªâ weighted sum)
- ‚úÖ Robust (√≠t overfitting)
- ‚úÖ Interpretable (bi·∫øt model n√†o quan tr·ªçng h∆°n)

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå Linear combination only (kh√¥ng h·ªçc ƒë∆∞·ª£c non-linear relationships)
- ‚ùå Kh√¥ng t·∫≠n d·ª•ng ƒë∆∞·ª£c interaction gi·ªØa predictions
- ‚ùå T·∫•t c·∫£ impressions d√πng c√πng weights (kh√¥ng adaptive)

---

## 2. STACKING ENSEMBLE

### 2.1. √ù t∆∞·ªüng c∆° b·∫£n

Stacking s·ª≠ d·ª•ng m·ªôt **meta-model** (LightGBM) ƒë·ªÉ h·ªçc c√°ch k·∫øt h·ª£p predictions:

```
Base predictions ‚Üí Feature Engineering ‚Üí Meta-model ‚Üí Final prediction
```

**Kh√°c v·ªõi Weighted Mean:**
- Weighted: P = w‚ÇÅP‚ÇÅ + w‚ÇÇP‚ÇÇ (linear)
- Stacking: P = f(P‚ÇÅ, P‚ÇÇ, features) (non-linear, ph·ª©c t·∫°p h∆°n)

### 2.2. Feature Engineering

Kh√¥ng ch·ªâ d√πng raw predictions [P‚ÇÅ, P‚ÇÇ], m√† **t·∫°o th√™m nhi·ªÅu features** t·ª´ ch√∫ng:

#### 2.2.1. Statistical Features (trong impression)
```
Cho m·ªói prediction P:
  - P_mean: Mean c·ªßa P trong impression
  - P_max: Max c·ªßa P trong impression  
  - P_min: Min c·ªßa P trong impression
  - P_std: Standard deviation c·ªßa P trong impression
```

**V√≠ d·ª•:** Impression c√≥ 5 candidates v·ªõi P‚ÇÅ = [0.8, 0.3, 0.6, 0.2, 0.9]
```
P‚ÇÅ_mean = 0.56
P‚ÇÅ_max = 0.9
P‚ÇÅ_min = 0.2
P‚ÇÅ_std = 0.28
```

#### 2.2.2. Normalized Features
```
P_zscore = (P - P_mean) / (P_std + Œµ)
P_normed = (P - P_min) / (P_max - P_min + Œµ)
```

**√ù nghƒ©a:**
- Z-score: Bao nhi√™u standard deviations t·ª´ mean?
- Normalized: V·ªã tr√≠ t∆∞∆°ng ƒë·ªëi trong [0,1]

#### 2.2.3. Rank Features
```
P_rank = Rank of P within impression (ascending)
P_rank_desc = Rank of P within impression (descending)
P_normedrank = P_rank / impression_size
P_normedrank_desc = P_rank_desc / impression_size
```

**V√≠ d·ª•:** P = [0.8, 0.3, 0.6, 0.2, 0.9]
```
P_rank_desc = [2, 4, 3, 5, 1]
P_normedrank_desc = [0.4, 0.8, 0.6, 1.0, 0.2]
```

#### 2.2.4. Pairwise Features (gi·ªØa 2 models)
```
diff = P‚ÇÅ - P‚ÇÇ
ratio = P‚ÇÅ / (P‚ÇÇ + Œµ)
max = max(P‚ÇÅ, P‚ÇÇ)
min = min(P‚ÇÅ, P‚ÇÇ)
```

**√ù nghƒ©a:** Capture relationships gi·ªØa predictions c·ªßa 2 models

#### 2.2.5. Aggregate Features
```
pred_mean_all = Mean c·ªßa t·∫•t c·∫£ predictions
impression_count = S·ªë candidates trong impression
```

**T·ªïng k·∫øt features:**
- 2 base predictions ‚Üí ~100+ engineered features
- M·ªói feature capture m·ªôt kh√≠a c·∫°nh kh√°c nhau c·ªßa data

### 2.3. Meta-model: LightGBM v·ªõi LambdaRank

#### T·∫°i sao LightGBM?
- üéØ Gradient Boosting: M·∫°nh v·ªõi tabular data
- üéØ LambdaRank objective: ƒê∆∞·ª£c thi·∫øt k·∫ø CHO RANKING tasks
- üéØ Nhanh, efficient v·ªõi nhi·ªÅu features

#### LambdaRank Objective

**Ranking problem:**
- Kh√¥ng ch·ªâ predict label (0/1)
- M√† predict **th·ª© t·ª±** ƒë√∫ng c·ªßa candidates

**LambdaRank:**
- T·ªëi ∆∞u h√≥a **ranking metrics** (NDCG@k) tr·ª±c ti·∫øp
- H·ªçc c√°ch s·∫Øp x·∫øp candidates ƒë√∫ng th·ª© t·ª±
- T√≠nh gradient d·ª±a tr√™n pairwise comparisons

**NDCG@k (Normalized Discounted Cumulative Gain):**
```
DCG@k = Œ£·µ¢‚Çå‚ÇÅ·µè (2^rel·µ¢ - 1) / log‚ÇÇ(i+1)
NDCG@k = DCG@k / IDCG@k
```
- ƒê√°nh gi√° cao vi·ªác rank ƒë√∫ng items quan tr·ªçng (clicked)
- Gi·∫£m tr·ªçng s·ªë theo position (position 1 > position 10)

#### LightGBM Parameters
```python
lgb_params = {
    'objective': 'lambdarank',      # Ranking objective
    'metric': 'ndcg',                # Optimize NDCG
    'ndcg_at': [5, 10],             # Evaluate at top-5, top-10
    'learning_rate': 0.1,            # Learning rate
    'feature_fraction': 0.8,         # Use 80% features per tree
    'bagging_fraction': 0.8,         # Use 80% samples per iteration
    'bagging_freq': 1,               # Bagging every iteration
    'max_bin': 1024,                 # Max bins for features
}
```

### 2.4. Cross-Validation: GroupKFold

**V·∫•n ƒë·ªÅ:** Candidates trong c√πng impression c√≥ correlation

**Gi·∫£i ph√°p:** GroupKFold
```
Fold 1: Train on impressions [1000-5000], Valid on [0-1000]
Fold 2: Train on [0-1000, 2000-5000], Valid on [1000-2000]
...
Fold k: Train on other folds, Valid on this fold
```

**Quan tr·ªçng:**
- T·∫•t c·∫£ candidates c·ªßa 1 impression ph·∫£i trong C√ôNG fold
- Tr√°nh data leakage gi·ªØa train/valid

### 2.5. Out-of-Fold (OOF) Predictions

```
Training flow:
  Fold 1: Train model‚ÇÅ, predict on valid_fold‚ÇÅ ‚Üí oof_pred‚ÇÅ
  Fold 2: Train model‚ÇÇ, predict on valid_fold‚ÇÇ ‚Üí oof_pred‚ÇÇ
  ...
  Fold k: Train model‚Çñ, predict on valid_fold‚Çñ ‚Üí oof_pred‚Çñ
  
  Combine: oof_predictions = [oof_pred‚ÇÅ, oof_pred‚ÇÇ, ..., oof_pred‚Çñ]
  Calculate: OOF_AUC = AUC(y_true, oof_predictions)
```

**OOF AUC:**
- ƒê√°nh gi√° **unbiased** performance
- M·ªói sample ƒë∆∞·ª£c predict b·ªüi model CH∆ØA th·∫•y n√≥ trong training
- G·∫ßn v·ªõi test performance h∆°n train performance

### 2.6. Inference (Prediction)

```
For new data:
  1. Create features (same engineering)
  2. Predict v·ªõi M·ªñI fold model:
     pred‚ÇÅ = model‚ÇÅ.predict(features)
     pred‚ÇÇ = model‚ÇÇ.predict(features)
     ...
     pred‚Çñ = model‚Çñ.predict(features)
  3. Average predictions:
     final_pred = (pred‚ÇÅ + pred‚ÇÇ + ... + pred‚Çñ) / k
```

**Averaging k models:**
- Gi·∫£m variance
- Robust h∆°n single model
- Exploit diversity gi·ªØa c√°c folds

### 2.7. ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm

**∆Øu ƒëi·ªÉm:**
- ‚úÖ Non-linear combination (h·ªçc relationships ph·ª©c t·∫°p)
- ‚úÖ Feature engineering ‚Üí capture nhi·ªÅu patterns
- ‚úÖ LambdaRank ‚Üí optimize directly cho ranking
- ‚úÖ Th∆∞·ªùng performance cao h∆°n Weighted Mean

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå Ph·ª©c t·∫°p h∆°n (nhi·ªÅu b∆∞·ªõc, nhi·ªÅu hyperparameters)
- ‚ùå Ch·∫≠m h∆°n (training + feature engineering)
- ‚ùå D·ªÖ overfit h∆°n (c·∫ßn careful tuning)
- ‚ùå √çt interpretable (black-box model)

---

## 3. SO S√ÅNH WEIGHTED MEAN VS STACKING

| Ti√™u ch√≠ | Weighted Mean | Stacking |
|----------|---------------|----------|
| **Complexity** | Simple (weighted sum) | Complex (feature eng. + model) |
| **Speed** | R·∫•t nhanh | Ch·∫≠m h∆°n |
| **Performance** | T·ªët | Th∆∞·ªùng t·ªët h∆°n |
| **Interpretability** | Cao (xem weights) | Th·∫•p (black-box) |
| **Overfitting risk** | Th·∫•p | Trung b√¨nh-Cao |
| **Training time** | Nhanh (~minutes) | Ch·∫≠m (~hours) |
| **Inference time** | R·∫•t nhanh | Nhanh |

---

## 4. KHI N√ÄO D√ôNG PH∆Ø∆†NG PH√ÅP N√ÄO?

### D√πng Weighted Mean khi:
- ‚úÖ C·∫ßn solution ƒë∆°n gi·∫£n, nhanh
- ‚úÖ Models t∆∞∆°ng ƒë·ªëi independent
- ‚úÖ Mu·ªën interpretability cao
- ‚úÖ √çt th·ªùi gian training
- ‚úÖ Dataset nh·ªè (d·ªÖ overfit v·ªõi stacking)

### D√πng Stacking khi:
- ‚úÖ C·∫ßn squeeze maximum performance
- ‚úÖ C√≥ ƒë·ªß data (tr√°nh overfit)
- ‚úÖ C√≥ time/resource ƒë·ªÉ train meta-model
- ‚úÖ Models c√≥ complementary strengths
- ‚úÖ Task ph·ª©c t·∫°p (non-linear relationships)

### Best Practice: Th·ª≠ C·∫¢ HAI
```
1. Start v·ªõi Weighted Mean (baseline)
2. Implement Stacking (push performance)
3. Compare results
4. Choose based on requirements (speed vs accuracy)
```

---

## 5. L∆ØU √ù QUAN TR·ªåNG

### 5.1. Data Leakage
- ‚ùå KH√îNG train meta-model tr√™n c√πng data d√πng ƒë·ªÉ train base models
- ‚úÖ D√πng separate validation set ho·∫∑c OOF predictions
- ‚úÖ GroupKFold ƒë·ªÉ tr√°nh leak gi·ªØa candidates trong impression

### 5.2. Overfitting
- ‚ö†Ô∏è Stacking d·ªÖ overfit n·∫øu kh√¥ng careful
- ‚úÖ Monitor OOF AUC vs validation AUC
- ‚úÖ Regularization (max_depth, min_samples, etc.)
- ‚úÖ Early stopping

### 5.3. Diversity c·ªßa Base Models
- üí° Ensemble ho·∫°t ƒë·ªông t·ªët khi base models **diverse**
- üí° Models qu√° gi·ªëng nhau ‚Üí ensemble kh√¥ng gi√∫p nhi·ªÅu
- üí° Check correlation matrix gi·ªØa predictions

### 5.4. Computational Cost
- ‚è±Ô∏è Weighted: O(n) - linear v·ªõi data size
- ‚è±Ô∏è Stacking: O(n √ó k √ó trees) - ph·ª• thu·ªôc nhi·ªÅu factors
- üí° Trade-off gi·ªØa accuracy gain vs computational cost

---

## 6. TO√ÅN H·ªåC CHI TI·∫æT

### 6.1. Weighted Mean Optimization

**Problem:**
```
maximize: E_impressions[AUC(y_true, Œ£·µ¢ w·µ¢P·µ¢)]
subject to: 0 ‚â§ w·µ¢ ‚â§ 1
```

**Optuna s·ª≠ d·ª•ng:**
- Tree-structured Parzen Estimator (TPE)
- M√¥ h√¨nh h√≥a P(score|weights) v√† P(weights)
- Ch·ªçn weights maximize P(weights|score > threshold)

### 6.2. LambdaRank Gradient

**Pairwise ranking:**
```
For pair (i, j) where rel·µ¢ > rel‚±º:
  Œª·µ¢‚±º = -‚àÇC / ‚àÇs·µ¢
  
  Where C = cost function based on ranking metric
  s·µ¢ = model score for item i
```

**Update:**
```
s·µ¢ ‚Üê s·µ¢ + Œ∑ √ó Œ£‚±º Œª·µ¢‚±º
```

**NDCG gradient:**
- T√≠nh impact n·∫øu swap positions i, j
- Gradient ‚àù |ŒîNDCG| √ó sigmoid(s·µ¢ - s‚±º)

---

## 7. K·∫æT LU·∫¨N

**Weighted Mean Ensemble:**
- Ph∆∞∆°ng ph√°p ƒë∆°n gi·∫£n nh∆∞ng hi·ªáu qu·∫£
- T·ªëi ∆∞u weights b·∫±ng Bayesian Optimization
- Ph√π h·ª£p l√†m baseline v√† production nhanh

**Stacking Ensemble:**
- Ph∆∞∆°ng ph√°p m·∫°nh m·∫Ω v·ªõi feature engineering
- Meta-model (LightGBM + LambdaRank) h·ªçc non-linear combinations
- Th∆∞·ªùng cho performance t·ªët nh·∫•t nh∆∞ng ph·ª©c t·∫°p h∆°n

**Recommendation:**
- Development: Implement C·∫¢ HAI
- Production: Choose d·ª±a tr√™n trade-off accuracy vs latency
- Best practice: Weighted cho speed, Stacking cho accuracy
